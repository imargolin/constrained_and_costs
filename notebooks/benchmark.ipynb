{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Danit and Chen, in this notebook I'll show you how to use the consept of \"Constraint\" how to use benchmark algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The right importing order is first the libraries, then the local files\n",
    "import sys; sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import ClassifierMixin\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from ada_csl_wrc.utils import filter_only_worst_features\n",
    "from ada_csl_wrc import Constraint, AbsoluteConstraint, RelativeConstraint\n",
    "\n",
    "from ada_csl_wrc.models import ConstrainedXGB\n",
    "from ada_csl_wrc.utils import benchmark_algorithm, is_satisfy_constraint\n",
    "\n",
    "from ada_csl_wrc.evaluation import evaluate\n",
    "from ada_csl_wrc.logger import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger=get_logger(\"Notebook\")\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/marketing_campaign.csv', sep=\";\")\n",
    "df = df.drop(['Z_CostContact', 'Z_Revenue', 'Income', 'Dt_Customer', 'ID'], axis = 1)\n",
    "full_X = df.drop(labels = 'Response', axis=1)\n",
    "y = df['Response']\n",
    "\n",
    "#Transforming categorial features into numerical\n",
    "categorial_col = full_X.select_dtypes(include='object').columns\n",
    "full_X[categorial_col] = full_X[categorial_col].astype('category').apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some configurations:\n",
    "\n",
    "#Only 3.75% of the population can be positive (at most), it is equivalent to 25% of 15%\n",
    "CONSTRAINT_RATIO = 0.25 * y.mean()\n",
    "constraint = RelativeConstraint(global_constraint=CONSTRAINT_RATIO)\n",
    "\n",
    "FEATURES_RATIO = 0.50\n",
    "COST_MATRIX = np.array([[ 0,  1],\n",
    "                        [10,  0]])\n",
    "\n",
    "X = filter_only_worst_features(full_X, y, FEATURES_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_constrainted_experiment(model: ClassifierMixin,\n",
    "                                X: np.ndarray,\n",
    "                                y: np.ndarray,\n",
    "                                cost_matrix: np.ndarray,\n",
    "                                constraint = Constraint,\n",
    "                                random_state = 42,\n",
    "                                n_splits=3):\n",
    "    fit_params = {}\n",
    "    out = {}\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for (fold, (train_index, test_index)) in enumerate(kf.split(X, y)):\n",
    "\n",
    "        #The ordinary Kfold, but with the constraint\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train, **fit_params)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        #Transforming the probabilities into 0 and 1 according to the constraint\n",
    "        y_pred = benchmark_algorithm(y_pred=y_pred_proba, group_ids=None, constraint=constraint) \n",
    "\n",
    "        out[fold] = evaluate(y_test, y_pred, cost_matrix)\n",
    "        logger.debug(f\"Number of positives: {y_pred.sum()}\")\n",
    "        logger.debug(f\"Ratio of positives: {y_pred.mean()}\")\n",
    "        logger.debug(f\"constraint: {constraint.to_dict()}\")\n",
    "    return pd.DataFrame(out).T.mean(axis=0).to_dict()\n",
    "\n",
    "\n",
    " #You can use AbsoluteConstraint or RelativeConstraint\n",
    "\n",
    "#run_constrainted_experiment(DecisionTreeClassifier(**dt_best_params), full_X.values, y.values, COST_MATRIX, constraint=constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2023-12-12 21:21:11,973 - Notebook - DEBUG - Number of positives: 27\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:11,973 - Notebook - DEBUG - Ratio of positives: 0.03614457831325301\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:11,974 - Notebook - DEBUG - constraint: {'global_constraint': 0.037276785714285714}\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:12,151 - Notebook - DEBUG - Number of positives: 27\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:12,152 - Notebook - DEBUG - Ratio of positives: 0.03614457831325301\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:12,153 - Notebook - DEBUG - constraint: {'global_constraint': 0.037276785714285714}\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:12,329 - Notebook - DEBUG - Number of positives: 27\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:12,330 - Notebook - DEBUG - Ratio of positives: 0.036193029490616625\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:21:12,331 - Notebook - DEBUG - constraint: {'global_constraint': 0.037276785714285714}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost': 971.6666666666666,\n",
       " 'accuracy': 0.8558033145390618,\n",
       " 'precision': 0.5679012345679012,\n",
       " 'recall': 0.13773595023595023,\n",
       " 'f1': 0.22170090014944566,\n",
       " 'g_mean': 0.36765701230716524}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_constrainted_experiment(XGBClassifier(max_depth=5, scale_pos_weight=1e1),\n",
    "                            X.values, y.values, COST_MATRIX, constraint=constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_XGB_experiment(model: ClassifierMixin,\n",
    "                       X: np.ndarray,\n",
    "                       y: np.ndarray,\n",
    "                       cost_matrix: np.ndarray,\n",
    "                       constraint = Constraint,\n",
    "                       random_state = 42,\n",
    "                       n_splits=3):\n",
    "    \n",
    "    \n",
    "    \n",
    "    fit_params = {}\n",
    "    out = {}\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for (fold, (train_index, test_index)) in enumerate(kf.split(X, y)):\n",
    "        model = ConstrainedXGB(XGBClassifier(), constraint=constraint)\n",
    "        #The ordinary Kfold, but with the constraint\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train, X_test, **fit_params)\n",
    "        y_pred = model.predict(X_test)\n",
    "        assert is_satisfy_constraint(y_pred, constraint)\n",
    "        out[fold] = evaluate(y_test, y_pred, cost_matrix)\n",
    "    return pd.DataFrame(out).T.mean(axis=0).to_dict()\n",
    "\n",
    "\n",
    " #You can use AbsoluteConstraint or RelativeConstraint\n",
    "\n",
    "#run_constrainted_experiment(DecisionTreeClassifier(**dt_best_params), full_X.values, y.values, COST_MATRIX, constraint=constraint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2023-12-12 21:22:02,563 - ada_csl_wrc.models - INFO - Finding the best estimator from history\u001b[0m\n",
      "\u001b[38;20m2023-12-12 21:22:02,563 - ada_csl_wrc.models - INFO - Finding the best estimator from history\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cost': 989.6666666666666,\n",
       " 'accuracy': 0.8517878484447173,\n",
       " 'precision': 0.5132953466286799,\n",
       " 'recall': 0.12277456027456028,\n",
       " 'f1': 0.19814475300115353,\n",
       " 'g_mean': 0.3465871945292354}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_XGB_experiment(XGBClassifier(max_depth=5, scale_pos_weight=1e1),\n",
    "                            X.values, y.values, COST_MATRIX, constraint=constraint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
